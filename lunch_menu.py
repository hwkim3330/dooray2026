#!/usr/bin/env python3
"""
íŒêµ í…Œí¬ë…¸ë°¸ë¦¬ êµ¬ë‚´ì‹ë‹¹ ì ì‹¬ ë©”ë‰´ ì•Œë¦¼
ë§¤ì¼ 11ì‹œ ì˜¤ëŠ˜ì˜ ë©”ë‰´ ë‘ë ˆì´ë¡œ ì „ì†¡
"""

import requests
from bs4 import BeautifulSoup
import subprocess
import os
import re
from datetime import datetime
from pathlib import Path
import urllib.parse
import pytesseract
from PIL import Image
import io

# ì„¤ì •
DOORAY_WEBHOOK = "https://keti.dooray.com/services/3711006199900720461/4145226571364668339/1QmQmcTCTMKf3FyF1OemZA"
MENU_DIR = Path("/home/kim/dooray-claude-bot/menus")
MENU_DIR.mkdir(exist_ok=True)

# íŒêµ í…Œí¬ë…¸ë°¸ë¦¬ ê³µì§€ì‚¬í•­
PANGYO_URL = "https://www.pangyotechnovalley.org/base/board/list?boardManagementNo=18&menuLevel=2&menuNo=55"

# ìš”ì¼
WEEKDAYS = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]

def get_latest_menu_post():
    """ìµœì‹  ì‹ë‹¨í‘œ ê²Œì‹œê¸€ ì°¾ê¸°"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(PANGYO_URL, headers=headers, timeout=30)
        soup = BeautifulSoup(resp.text, 'html.parser')

        # ê²Œì‹œê¸€ ëª©ë¡ì—ì„œ 'ì‹ë‹¨' í¬í•¨ëœ ìµœì‹  ê¸€ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            text = link.get_text()
            if 'ì‹ë‹¨' in text or 'ë©”ë‰´' in text:
                href = link['href']
                if 'boardNo=' in href:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ
                    if href.startswith('/'):
                        return f"https://www.pangyotechnovalley.org{href}"
                    return href

        return None
    except Exception as e:
        print(f"ê²Œì‹œê¸€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None

def download_attachments(post_url):
    """ê²Œì‹œê¸€ì—ì„œ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(post_url, headers=headers, timeout=30)
        soup = BeautifulSoup(resp.text, 'html.parser')

        files = []
        # ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link['href']
            text = link.get_text().lower()

            if any(ext in text or ext in href.lower() for ext in ['.pdf', '.png', '.jpg', '.jpeg']):
                if href.startswith('/'):
                    href = f"https://www.pangyotechnovalley.org{href}"

                filename = link.get_text().strip()
                if not filename:
                    filename = href.split('/')[-1]

                # ë‹¤ìš´ë¡œë“œ
                file_resp = requests.get(href, headers=headers, timeout=60)
                filepath = MENU_DIR / filename
                filepath.write_bytes(file_resp.content)
                files.append(filepath)
                print(f"ë‹¤ìš´ë¡œë“œ: {filename}")

        return files
    except Exception as e:
        print(f"ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

def ocr_image(image_path):
    """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)"""
    try:
        img = Image.open(image_path)
        text = pytesseract.image_to_string(img, lang='kor+eng')
        return text
    except Exception as e:
        print(f"OCR ì˜¤ë¥˜: {e}")
        return ""

def ocr_pdf(pdf_path):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        import tempfile
        with tempfile.TemporaryDirectory() as tmpdir:
            # pdftoppmìœ¼ë¡œ ì´ë¯¸ì§€ ë³€í™˜
            subprocess.run([
                'pdftoppm', '-png', '-r', '200',
                str(pdf_path), f'{tmpdir}/page'
            ], check=True, timeout=60)

            # ë³€í™˜ëœ ì´ë¯¸ì§€ë“¤ OCR
            text = ""
            for img_path in sorted(Path(tmpdir).glob('*.png')):
                text += ocr_image(img_path) + "\n"

            return text
    except Exception as e:
        print(f"PDF OCR ì˜¤ë¥˜: {e}")
        return ""

def extract_menu_text(filepath):
    """íŒŒì¼ì—ì„œ ë©”ë‰´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    suffix = filepath.suffix.lower()

    if suffix == '.pdf':
        return ocr_pdf(filepath)
    elif suffix in ['.png', '.jpg', '.jpeg']:
        return ocr_image(filepath)
    else:
        return ""

def parse_today_menu(text, cafeteria_name=""):
    """í…ìŠ¤íŠ¸ì—ì„œ ì˜¤ëŠ˜ ë©”ë‰´ ì¶”ì¶œ"""
    today = datetime.now()
    weekday = WEEKDAYS[today.weekday()]
    day = today.day

    # í…ìŠ¤íŠ¸ ì •ë¦¬
    lines = text.split('\n')
    lines = [l.strip() for l in lines if l.strip()]

    # ì˜¤ëŠ˜ ë‚ ì§œ/ìš”ì¼ ê·¼ì²˜ ë©”ë‰´ ì°¾ê¸°
    menu_lines = []
    found_today = False

    for i, line in enumerate(lines):
        # ì˜¤ëŠ˜ ì°¾ê¸° (ë‚ ì§œ ë˜ëŠ” ìš”ì¼)
        if f"{day}ì¼" in line or f"({weekday})" in line or weekday in line:
            found_today = True
            # ë‹¤ìŒ ëª‡ ì¤„ì´ ë©”ë‰´ì¼ ê°€ëŠ¥ì„±
            for j in range(i, min(i+10, len(lines))):
                menu_lines.append(lines[j])

            # ë‹¤ìŒ ìš”ì¼ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
            next_weekday = WEEKDAYS[(today.weekday() + 1) % 7]
            if next_weekday in lines[j] if j < len(lines) else False:
                break

    if menu_lines:
        return "\n".join(menu_lines)

    # ëª» ì°¾ìœ¼ë©´ AIì—ê²Œ ìš”ì²­
    return None

def extract_with_ai(text, cafeteria_name):
    """AIë¡œ ì˜¤ëŠ˜ ë©”ë‰´ ì¶”ì¶œ"""
    today = datetime.now()
    weekday = WEEKDAYS[today.weekday()]
    date_str = today.strftime("%mì›” %dì¼")

    prompt = f"""ë‹¤ìŒ ì‹ë‹¨í‘œì—ì„œ ì˜¤ëŠ˜({date_str} {weekday}ìš”ì¼) ì ì‹¬ ë©”ë‰´ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ì‹ë‹¹: {cafeteria_name}

ì‹ë‹¨í‘œ:
{text[:2000]}

í˜•ì‹:
- ë©”ë‰´1
- ë©”ë‰´2
...

ì˜¤ëŠ˜ ë©”ë‰´ê°€ ì—†ìœ¼ë©´ "ë©”ë‰´ ì •ë³´ ì—†ìŒ"ì´ë¼ê³  í•´ì£¼ì„¸ìš”."""

    try:
        result = subprocess.run(
            ["claude", "-p", prompt, "--model", "haiku"],
            capture_output=True,
            text=True,
            timeout=30,
            env={**os.environ, "LANG": "ko_KR.UTF-8"}
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except:
        pass

    return "ë©”ë‰´ ì¶”ì¶œ ì‹¤íŒ¨"

def get_all_menus():
    """ëª¨ë“  ì‹ë‹¹ ë©”ë‰´ ê°€ì ¸ì˜¤ê¸°"""
    menus = {}

    # 1. íŒêµ í…Œí¬ë…¸ë°¸ë¦¬ (PDF/PNG)
    print("íŒêµ í…Œí¬ë…¸ë°¸ë¦¬ ë©”ë‰´ í™•ì¸ ì¤‘...")
    post_url = get_latest_menu_post()

    if post_url:
        print(f"ê²Œì‹œê¸€: {post_url}")
        files = download_attachments(post_url)

        for filepath in files:
            name = filepath.stem  # íŒŒì¼ëª…ì—ì„œ ì‹ë‹¹ëª… ì¶”ì¶œ
            print(f"OCR ì²˜ë¦¬ ì¤‘: {name}")

            text = extract_menu_text(filepath)
            if text:
                menu = extract_with_ai(text, name)
                menus[name] = menu
    else:
        print("ì‹ë‹¨í‘œ ê²Œì‹œê¸€ ëª» ì°¾ìŒ")

    return menus

def send_to_dooray(menus):
    """ë‘ë ˆì´ë¡œ ì ì‹¬ ë©”ë‰´ ì „ì†¡"""
    today = datetime.now()
    date_str = today.strftime("%mì›” %dì¼")
    weekday = WEEKDAYS[today.weekday()]

    if not menus:
        menu_text = "ì˜¤ëŠ˜ ì‹ë‹¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    else:
        menu_text = ""
        for cafeteria, menu in menus.items():
            menu_text += f"ğŸ½ï¸ {cafeteria}\n{menu}\n\n"

    message = f"""ğŸ´ {date_str} ({weekday}) ì˜¤ëŠ˜ì˜ ì ì‹¬

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{menu_text.strip()}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ë§›ìˆëŠ” ì ì‹¬ ë˜ì„¸ìš”! ğŸš
"""

    payload = {
        "botName": "ì ì‹¬ë©”ë‰´",
        "botIconImage": "https://em-content.zobj.net/source/apple/391/fork-and-knife_1f374.png",
        "text": message
    }

    try:
        resp = requests.post(DOORAY_WEBHOOK, json=payload, timeout=10)
        print(f"ë‘ë ˆì´ ì „ì†¡: {resp.status_code}")
        return resp.status_code == 200
    except Exception as e:
        print(f"ì „ì†¡ ì‹¤íŒ¨: {e}")
        return False

def main():
    print(f"[{datetime.now()}] ì ì‹¬ ë©”ë‰´ ë´‡ ì‹œì‘")

    # ì£¼ë§ ì²´í¬
    if datetime.now().weekday() >= 5:
        print("ì£¼ë§ì€ ì‰½ë‹ˆë‹¤~")
        return

    # ë©”ë‰´ ê°€ì ¸ì˜¤ê¸°
    menus = get_all_menus()
    print(f"ë©”ë‰´ {len(menus)}ê°œ ì‹ë‹¹ ìˆ˜ì§‘")

    # ë‘ë ˆì´ ì „ì†¡
    send_to_dooray(menus)

if __name__ == "__main__":
    main()
